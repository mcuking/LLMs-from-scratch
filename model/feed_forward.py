import torch.nn as nn

# 前馈神经网络，包含三个层：
# 1. 一个线性层（nn.Linear），将输入维度从emb_dim扩展到4倍的emb_dim。
# 2. 一个GELU激活函数。
# 3. 一个线性层（nn.Linear），将输出维度从4倍的emb_dim压缩回emb_dim。

# 前馈神经网络在Transformer架构中通常用于每个注意力层之后，对注意力输出进行进一步的处理。作用：
# 1. 引入非线性：虽然注意力机制本身已经具有非线性（因为使用了softmax），
# 但增加一个带有激活函数的前馈网络可以进一步增强模型的非线性表达能力，从而学习更复杂的函数；
# 2. 增加模型的容量：通过扩展维度（这里扩展到4倍）然后再压缩回来，实际上增加了模型的参数数量，使得模型能够学习更复杂的模式；
# 总结就是：通过维度扩展-激活-压缩结构增强表达能力。
class FeedForward(nn.Module):
    def __init__(self, emd_dim):
        """
        初始化前馈神经网络层

        Args:
            emd_dim (int): 输入张量的大小
        """
        super().__init__()

        # 为什么需要激活函数？  
        # 激活函数在这里是模型能够学习复杂非线性变换的关键，使得神经网络能够拟合任意复杂的函数。
        # 如果没有激活函数，那么FeedForward模块就变成了两个线性层的堆叠，即：
        # y = W2(W1*x + b1) + b2 = (W2*W1)*x + (W2*b1 + b2)
        # 这等价于一个线性变换，大大限制了模型的表达能力。

        # 为什么选择GELU而不是ReLU？
        # ReLU(x) = max(0, x)
        # ReLU 在输入正数时直接输出输入，否则输出0。
        # GELU(x) ≈ 0.5x(1 + tanh(√(2/π)(x + 0.044715x³)))
        # GELU 是一个平滑的非线性函数，近似于ReLU，但几乎所有负值上都有非零梯度。

        # GELU 的平滑特性可以运行模型参数进行更细微的调整，相比之下 ReLU 在零点处有一个尖锐的拐角，使得优化困难；
        # 此外 ReLU 对负输入的输出为0，而GELU对负输入会输出一个小的非零值，
        # 意味着训练中收到负输入的神经元仍然可以参与学习，只是贡献程度不如正输入大。
        self.layers = nn.Sequential(
            nn.Linear(emd_dim, 4 * emd_dim), # 第一个线性层：扩展维度
            nn.GELU(),  # 非线性激活函数
            nn.Linear(4 * emd_dim, emd_dim), # 第二个线性层：压缩回原维度
        )

    def forward(self, x):
        return self.layers(x)
